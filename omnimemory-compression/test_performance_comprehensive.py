#!/usr/bin/env python3
"""
Comprehensive Performance Test Suite for OmniMemory Compression Service

Tests:
1. Token counting across multiple models
2. Compression performance with caching
3. Cache hit rate verification
4. Validation system
5. Multi-model support
6. Before/After performance comparison
"""

import asyncio
import httpx
import time
from typing import Dict, List, Any
from dataclasses import dataclass
import sys


# ANSI color codes for pretty output
class Colors:
    HEADER = "\033[95m"
    OKBLUE = "\033[94m"
    OKCYAN = "\033[96m"
    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"


@dataclass
class TestResult:
    """Container for test results"""

    name: str
    passed: bool
    metrics: Dict[str, Any]
    message: str


class PerformanceTestSuite:
    """Comprehensive performance test suite"""

    def __init__(self, base_url: str = "http://localhost:8001"):
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=30.0)
        self.results: List[TestResult] = []

    async def close(self):
        """Close HTTP client"""
        await self.client.aclose()

    def print_header(self, text: str):
        """Print formatted header"""
        print(f"\n{Colors.BOLD}{Colors.HEADER}{'='*80}{Colors.ENDC}")
        print(f"{Colors.BOLD}{Colors.HEADER}{text:^80}{Colors.ENDC}")
        print(f"{Colors.BOLD}{Colors.HEADER}{'='*80}{Colors.ENDC}\n")

    def print_subheader(self, text: str):
        """Print formatted subheader"""
        print(f"\n{Colors.BOLD}{Colors.OKCYAN}{text}{Colors.ENDC}")
        print(f"{Colors.OKCYAN}{'-'*len(text)}{Colors.ENDC}")

    def print_result(self, passed: bool, message: str):
        """Print test result"""
        if passed:
            print(f"{Colors.OKGREEN}‚úì {message}{Colors.ENDC}")
        else:
            print(f"{Colors.FAIL}‚úó {message}{Colors.ENDC}")

    def print_metric(self, key: str, value: Any, unit: str = ""):
        """Print metric"""
        print(f"  {Colors.OKBLUE}{key:.<40}{Colors.ENDC} {value} {unit}")

    async def check_health(self) -> bool:
        """Verify service is healthy"""
        self.print_subheader("Service Health Check")

        try:
            response = await self.client.get(f"{self.base_url}/health")
            if response.status_code != 200:
                self.print_result(False, "Service not responding")
                return False

            health = response.json()
            self.print_result(True, f"Service: {health['service']}")
            self.print_metric("Status", health["status"])
            self.print_metric("Tokenizer Enabled", health["tokenizer_enabled"])
            self.print_metric("Cache Enabled", health["cache_enabled"])
            self.print_metric("Validator Enabled", health["validator_enabled"])

            return all(
                [
                    health["tokenizer_enabled"],
                    health["cache_enabled"],
                    health["validator_enabled"],
                ]
            )

        except Exception as e:
            self.print_result(False, f"Health check failed: {e}")
            return False

    async def test_token_counting(self) -> TestResult:
        """Test token counting across multiple models"""
        self.print_subheader("1. Token Counting Tests")

        # Test models
        models = [
            "gpt-4",
            "gpt-3.5-turbo",
            "claude-3-opus-20240229",
            "claude-3-sonnet-20240229",
            "llama-2-70b-chat",
        ]

        # Test texts
        test_texts = {
            "short": "Hello, world! " * 7,  # ~100 chars
            "medium": "The quick brown fox jumps over the lazy dog. "
            * 22,  # ~1000 chars
            "long": "This is a test sentence for token counting. " * 227,  # ~10K chars
        }

        results = {}
        total_tests = 0
        passed_tests = 0

        for model_id in models:
            print(f"\n  Testing model: {Colors.BOLD}{model_id}{Colors.ENDC}")
            model_results = {}

            for text_type, text in test_texts.items():
                try:
                    start_time = time.perf_counter()

                    response = await self.client.post(
                        f"{self.base_url}/count-tokens",
                        json={
                            "text": text,
                            "model_id": model_id,
                        },
                    )

                    elapsed_ms = (time.perf_counter() - start_time) * 1000
                    total_tests += 1

                    if response.status_code == 200:
                        data = response.json()
                        model_results[text_type] = {
                            "token_count": data["token_count"],
                            "strategy": data["strategy_used"],
                            "is_exact": data["is_exact"],
                            "response_time_ms": round(elapsed_ms, 2),
                        }

                        # Check if GPT models use exact tokenization (tiktoken)
                        if "gpt" in model_id.lower():
                            if data["strategy_used"] == "tiktoken":
                                passed_tests += 1
                                self.print_result(
                                    True,
                                    f"{text_type}: {data['token_count']} tokens (exact tiktoken) in {elapsed_ms:.1f}ms",
                                )
                            else:
                                self.print_result(
                                    False,
                                    f"{text_type}: Using {data['strategy_used']} instead of tiktoken",
                                )
                        else:
                            passed_tests += 1
                            self.print_result(
                                True,
                                f"{text_type}: {data['token_count']} tokens ({data['strategy_used']}) in {elapsed_ms:.1f}ms",
                            )
                    else:
                        self.print_result(
                            False, f"{text_type}: HTTP {response.status_code}"
                        )

                except Exception as e:
                    self.print_result(False, f"{text_type}: {str(e)}")

            results[model_id] = model_results

        metrics = {
            "models_tested": len(models),
            "text_sizes_tested": len(test_texts),
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "results": results,
        }

        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        return TestResult(
            name="Token Counting",
            passed=success_rate >= 80,
            metrics=metrics,
            message=f"Success rate: {success_rate:.1f}% ({passed_tests}/{total_tests})",
        )

    async def test_compression_performance(self) -> TestResult:
        """Test compression performance with caching"""
        self.print_subheader("2. Compression Performance Tests (FastCDC Caching)")

        # Create a realistic long context (20K+ characters)
        long_context = (
            """
        Artificial Intelligence (AI) has transformed modern society in profound ways.
        Machine learning models can now understand natural language, generate creative content,
        and solve complex problems. However, these models often require extensive context to
        perform optimally. Context compression techniques like VisionDrop help reduce token
        usage while maintaining semantic quality. This is crucial for cost optimization and
        enabling longer conversations within context windows.

        The FastCDC (Fast Content-Defined Chunking) algorithm plays a key role in caching.
        By identifying content boundaries based on data patterns rather than fixed positions,
        FastCDC enables efficient deduplication. When the same or similar content is compressed
        repeatedly, FastCDC can detect these patterns and retrieve cached results, leading to
        dramatic speedups of 10-50x for repeated content.

        """
            * 50
        )  # Repeat to create ~20K characters

        print(f"  Test document size: {len(long_context):,} characters")

        # Run compression 3 times
        timings = []
        compression_results = []

        for run in range(1, 4):
            print(f"\n  Run {run}:")

            try:
                start_time = time.perf_counter()

                response = await self.client.post(
                    f"{self.base_url}/compress",
                    json={
                        "context": long_context,
                        "target_compression": 0.5,  # 50% compression
                        "model_id": "gpt-4",
                    },
                )

                elapsed_ms = (time.perf_counter() - start_time) * 1000
                timings.append(elapsed_ms)

                if response.status_code == 200:
                    data = response.json()
                    compression_results.append(data)

                    self.print_metric("Time", f"{elapsed_ms:.1f}", "ms")
                    self.print_metric("Original tokens", data["original_tokens"])
                    self.print_metric("Compressed tokens", data["compressed_tokens"])
                    self.print_metric(
                        "Compression ratio", f"{data['compression_ratio']:.1%}"
                    )
                    self.print_metric("Quality score", f"{data['quality_score']:.1%}")

                    if run > 1:
                        speedup = timings[0] / elapsed_ms
                        self.print_metric("Speedup vs Run 1", f"{speedup:.1f}x")

                        if speedup >= 2.0:
                            self.print_result(
                                True, f"Cache speedup achieved: {speedup:.1f}x faster"
                            )
                        else:
                            self.print_result(
                                False, f"Expected >2x speedup, got {speedup:.1f}x"
                            )
                else:
                    self.print_result(False, f"HTTP {response.status_code}")

            except Exception as e:
                self.print_result(False, f"Compression failed: {e}")

        # Calculate metrics
        if len(timings) >= 3:
            avg_speedup_run2 = timings[0] / timings[1]
            avg_speedup_run3 = timings[0] / timings[2]
            max_speedup = max(avg_speedup_run2, avg_speedup_run3)
        else:
            max_speedup = 1.0

        metrics = {
            "document_size": len(long_context),
            "runs": len(timings),
            "timings_ms": timings,
            "speedup_run2": round(timings[0] / timings[1], 2)
            if len(timings) >= 2
            else 0,
            "speedup_run3": round(timings[0] / timings[2], 2)
            if len(timings) >= 3
            else 0,
            "max_speedup": round(max_speedup, 2),
            "compression_results": compression_results,
        }

        # Success if speedup >= 2x (conservative, looking for 10-50x)
        passed = max_speedup >= 2.0

        return TestResult(
            name="Compression Performance",
            passed=passed,
            metrics=metrics,
            message=f"Max speedup: {max_speedup:.1f}x (target: >2x)",
        )

    async def test_cache_performance(self) -> TestResult:
        """Test cache hit rate and performance"""
        self.print_subheader("3. Cache Performance Verification")

        try:
            # Get cache stats
            response = await self.client.get(f"{self.base_url}/cache/stats")

            if response.status_code != 200:
                return TestResult(
                    name="Cache Performance",
                    passed=False,
                    metrics={},
                    message=f"Cache stats unavailable: HTTP {response.status_code}",
                )

            cache_data = response.json()

            if not cache_data.get("cache_enabled"):
                return TestResult(
                    name="Cache Performance",
                    passed=False,
                    metrics={},
                    message="Cache not enabled",
                )

            stats = cache_data.get("stats", {})

            # Calculate hit rates
            l1_stats = stats.get("l1", {})
            l2_stats = stats.get("l2", {})

            l1_hits = l1_stats.get("hits", 0)
            l1_misses = l1_stats.get("misses", 0)
            l1_total = l1_hits + l1_misses
            l1_hit_rate = (l1_hits / l1_total * 100) if l1_total > 0 else 0

            l2_hits = l2_stats.get("hits", 0)
            l2_misses = l2_stats.get("misses", 0)
            l2_total = l2_hits + l2_misses
            l2_hit_rate = (l2_hits / l2_total * 100) if l2_total > 0 else 0

            # Overall hit rate
            total_hits = l1_hits + l2_hits
            total_requests = l1_total + l2_total
            overall_hit_rate = (
                (total_hits / total_requests * 100) if total_requests > 0 else 0
            )

            print(f"\n  L1 Cache (In-Memory):")
            self.print_metric("Hits", l1_hits)
            self.print_metric("Misses", l1_misses)
            self.print_metric("Hit Rate", f"{l1_hit_rate:.1f}%")
            self.print_metric("Size", l1_stats.get("size", 0))

            print(f"\n  L2 Cache (Disk):")
            self.print_metric("Hits", l2_hits)
            self.print_metric("Misses", l2_misses)
            self.print_metric("Hit Rate", f"{l2_hit_rate:.1f}%")
            self.print_metric("Size", l2_stats.get("size", 0))

            print(f"\n  Overall:")
            self.print_metric("Total Hits", total_hits)
            self.print_metric("Total Requests", total_requests)
            self.print_metric("Hit Rate", f"{overall_hit_rate:.1f}%")

            # Check if hit rate meets target (>80%)
            if overall_hit_rate >= 80.0:
                self.print_result(
                    True, f"Hit rate {overall_hit_rate:.1f}% exceeds target (80%)"
                )
            elif overall_hit_rate >= 50.0:
                self.print_result(
                    True, f"Hit rate {overall_hit_rate:.1f}% is good (target 80%)"
                )
            else:
                self.print_result(
                    False, f"Hit rate {overall_hit_rate:.1f}% below target (80%)"
                )

            metrics = {
                "l1_hit_rate": round(l1_hit_rate, 2),
                "l2_hit_rate": round(l2_hit_rate, 2),
                "overall_hit_rate": round(overall_hit_rate, 2),
                "total_hits": total_hits,
                "total_requests": total_requests,
                "l1_size": l1_stats.get("size", 0),
                "l2_size": l2_stats.get("size", 0),
            }

            # Pass if hit rate > 50% (relaxed from 80% for initial tests)
            passed = overall_hit_rate >= 50.0

            return TestResult(
                name="Cache Performance",
                passed=passed,
                metrics=metrics,
                message=f"Hit rate: {overall_hit_rate:.1f}% (target: 80%)",
            )

        except Exception as e:
            return TestResult(
                name="Cache Performance",
                passed=False,
                metrics={},
                message=f"Error: {str(e)}",
            )

    async def test_validation_system(self) -> TestResult:
        """Test validation system"""
        self.print_subheader("4. Validation System Tests")

        original = (
            "The quick brown fox jumps over the lazy dog. This is a test sentence."
        )
        compressed_good = "Quick brown fox jumps over lazy dog. Test sentence."
        compressed_poor = "Fox dog."

        results = {}

        # Test good compression
        print(f"\n  Testing high-quality compression:")
        try:
            response = await self.client.post(
                f"{self.base_url}/validate",
                json={
                    "original": original,
                    "compressed": compressed_good,
                    "metrics": ["rouge-l"],
                },
            )

            if response.status_code == 200:
                data = response.json()
                results["good_compression"] = data

                self.print_metric(
                    "ROUGE-L Score", f"{data.get('rouge_l_score', 0):.3f}"
                )
                self.print_metric("Passed", data.get("passed", False))

                if data.get("rouge_l_score", 0) >= 0.5:
                    self.print_result(True, "High-quality compression validated")
                else:
                    self.print_result(False, "Score lower than expected")
            else:
                self.print_result(False, f"HTTP {response.status_code}")

        except Exception as e:
            self.print_result(False, f"Error: {e}")

        # Test poor compression
        print(f"\n  Testing low-quality compression:")
        try:
            response = await self.client.post(
                f"{self.base_url}/validate",
                json={
                    "original": original,
                    "compressed": compressed_poor,
                    "metrics": ["rouge-l"],
                },
            )

            if response.status_code == 200:
                data = response.json()
                results["poor_compression"] = data

                self.print_metric(
                    "ROUGE-L Score", f"{data.get('rouge_l_score', 0):.3f}"
                )
                self.print_metric("Passed", data.get("passed", False))

                if data.get("rouge_l_score", 0) < 0.5:
                    self.print_result(
                        True, "Low-quality compression correctly detected"
                    )
                else:
                    self.print_result(False, "Should have detected low quality")
            else:
                self.print_result(False, f"HTTP {response.status_code}")

        except Exception as e:
            self.print_result(False, f"Error: {e}")

        metrics = {
            "results": results,
            "good_quality_detected": results.get("good_compression", {}).get(
                "passed", False
            ),
            "poor_quality_detected": not results.get("poor_compression", {}).get(
                "passed", True
            ),
        }

        passed = metrics["good_quality_detected"] and metrics["poor_quality_detected"]

        return TestResult(
            name="Validation System",
            passed=passed,
            metrics=metrics,
            message="ROUGE-L validation working"
            if passed
            else "Validation issues detected",
        )

    async def test_multi_model_support(self) -> TestResult:
        """Test multi-model support"""
        self.print_subheader("5. Multi-Model Support Tests")

        test_text = "This is a test of multi-model token counting."

        model_families = {
            "GPT": ["gpt-4", "gpt-3.5-turbo"],
            "Claude": ["claude-3-opus-20240229", "claude-3-sonnet-20240229"],
            "Llama": ["llama-2-70b-chat"],
        }

        results = {}
        total_models = 0
        working_models = 0

        for family, models in model_families.items():
            print(f"\n  Testing {family} family:")
            family_results = {}

            for model_id in models:
                total_models += 1

                try:
                    response = await self.client.post(
                        f"{self.base_url}/count-tokens",
                        json={
                            "text": test_text,
                            "model_id": model_id,
                        },
                    )

                    if response.status_code == 200:
                        data = response.json()
                        family_results[model_id] = data
                        working_models += 1

                        self.print_result(
                            True,
                            f"{model_id}: {data['token_count']} tokens ({data['strategy_used']})",
                        )
                    else:
                        self.print_result(
                            False, f"{model_id}: HTTP {response.status_code}"
                        )

                except Exception as e:
                    self.print_result(False, f"{model_id}: {str(e)}")

            results[family] = family_results

        metrics = {
            "model_families": len(model_families),
            "total_models": total_models,
            "working_models": working_models,
            "results": results,
        }

        success_rate = (working_models / total_models * 100) if total_models > 0 else 0
        passed = success_rate >= 80

        return TestResult(
            name="Multi-Model Support",
            passed=passed,
            metrics=metrics,
            message=f"{working_models}/{total_models} models working ({success_rate:.0f}%)",
        )

    def print_summary(self):
        """Print final test summary"""
        self.print_header("TEST SUMMARY")

        total_tests = len(self.results)
        passed_tests = sum(1 for r in self.results if r.passed)

        print(f"\n{Colors.BOLD}Results:{Colors.ENDC}")
        print(f"{Colors.OKBLUE}{'='*80}{Colors.ENDC}")

        for result in self.results:
            status = (
                f"{Colors.OKGREEN}PASS{Colors.ENDC}"
                if result.passed
                else f"{Colors.FAIL}FAIL{Colors.ENDC}"
            )
            print(f"  [{status}] {result.name:.<50} {result.message}")

        print(f"{Colors.OKBLUE}{'='*80}{Colors.ENDC}")

        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        print(
            f"\n{Colors.BOLD}Overall Success Rate:{Colors.ENDC} {success_rate:.1f}% ({passed_tests}/{total_tests})"
        )

        # Print key improvements
        self.print_header("KEY IMPROVEMENTS VERIFIED")

        # Cache performance
        cache_result = next(
            (r for r in self.results if r.name == "Cache Performance"), None
        )
        if cache_result and cache_result.passed:
            hit_rate = cache_result.metrics.get("overall_hit_rate", 0)
            print(
                f"{Colors.OKGREEN}‚úì Cache hit rate: {hit_rate:.1f}% (was 0% before dependencies){Colors.ENDC}"
            )

        # Compression speedup
        comp_result = next(
            (r for r in self.results if r.name == "Compression Performance"), None
        )
        if comp_result and comp_result.passed:
            speedup = comp_result.metrics.get("max_speedup", 0)
            print(
                f"{Colors.OKGREEN}‚úì FastCDC speedup: {speedup:.1f}x faster on repeated content{Colors.ENDC}"
            )

        # Token counting
        token_result = next(
            (r for r in self.results if r.name == "Token Counting"), None
        )
        if token_result and token_result.passed:
            success = token_result.metrics.get("passed_tests", 0)
            total = token_result.metrics.get("total_tests", 0)
            print(
                f"{Colors.OKGREEN}‚úì Exact tokenization: {success}/{total} tests passed (tiktoken for GPT models){Colors.ENDC}"
            )

        # Multi-model support
        model_result = next(
            (r for r in self.results if r.name == "Multi-Model Support"), None
        )
        if model_result and model_result.passed:
            working = model_result.metrics.get("working_models", 0)
            total = model_result.metrics.get("total_models", 0)
            print(
                f"{Colors.OKGREEN}‚úì Multi-model support: {working}/{total} model families working{Colors.ENDC}"
            )

        # Validation
        val_result = next(
            (r for r in self.results if r.name == "Validation System"), None
        )
        if val_result and val_result.passed:
            print(
                f"{Colors.OKGREEN}‚úì ROUGE-L validation: Quality validation system operational{Colors.ENDC}"
            )

        print()

        # Final verdict
        if success_rate == 100:
            print(
                f"{Colors.BOLD}{Colors.OKGREEN}üéâ ALL TESTS PASSED - PRODUCTION READY{Colors.ENDC}"
            )
        elif success_rate >= 80:
            print(
                f"{Colors.BOLD}{Colors.WARNING}‚ö†Ô∏è  MOST TESTS PASSED - MINOR ISSUES{Colors.ENDC}"
            )
        else:
            print(
                f"{Colors.BOLD}{Colors.FAIL}‚ùå TESTS FAILED - NEEDS ATTENTION{Colors.ENDC}"
            )

        print()

    async def run_all_tests(self):
        """Run all performance tests"""
        self.print_header("OMNIMEMORY COMPRESSION SERVICE - PERFORMANCE TEST SUITE")

        # Health check
        if not await self.check_health():
            print(
                f"\n{Colors.FAIL}Service health check failed. Cannot proceed with tests.{Colors.ENDC}\n"
            )
            return

        # Run all test suites
        self.results.append(await self.test_token_counting())
        self.results.append(await self.test_compression_performance())
        self.results.append(await self.test_cache_performance())
        self.results.append(await self.test_validation_system())
        self.results.append(await self.test_multi_model_support())

        # Print summary
        self.print_summary()


async def main():
    """Main entry point"""
    suite = PerformanceTestSuite()

    try:
        await suite.run_all_tests()
    finally:
        await suite.close()


if __name__ == "__main__":
    asyncio.run(main())
